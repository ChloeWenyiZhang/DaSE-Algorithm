{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.baidu.com/ns?word=自然语言处理&tn=news&from=news&cl=2&rn=20&ct=1\n",
      "http://www.baidu.com/s?rtt=1&bsst=1&cl=2&tn=news&rsv_dl=ns_pc&word=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86&x_bfe_rqs=03E8000000000000000044&x_bfe_tjscore=0.100000&tngroupname=organic_news&newVideo=12&goods_entry_switch=1&pn=10\n",
      "10\n",
      "http://yuanchuang.10jqka.com.cn/20211223/c635373551.shtml\n",
      "输入成功！\n",
      "http://news.10jqka.com.cn/20211223/c635373551.shtml\n",
      "输入成功！\n",
      "https://www.163.com/dy/article/GRR7OG5J051480KF.html\n",
      "输入成功！\n",
      "http://dy.163.com/article/GRTCGB4505340BZM.html\n",
      "输入成功！\n",
      "http://stock.stockstar.com/RB2021122300007616.shtml\n",
      "输入成功！\n",
      "https://baijiahao.baidu.com/s?id=1719007876425418276&wfr=spider&for=pc\n",
      "输入成功！\n",
      "http://k.sina.com.cn/article_7628337882_1c6af32da001010ptr.html\n",
      "输入成功！\n",
      "https://baijiahao.baidu.com/s?id=1719178014796245420&wfr=spider&for=pc\n",
      "输入成功！\n",
      "http://baijiahao.baidu.com/s?id=1718935882272316070&wfr=spider&for=pc\n",
      "输入成功！\n",
      "http://baijiahao.baidu.com/s?id=1718996625200801734&wfr=spider&for=pc\n",
      "输入成功！\n",
      "http://www.baidu.com/s?rtt=1&bsst=1&cl=2&tn=news&rsv_dl=ns_pc&word=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86&x_bfe_rqs=03E8000000000000000022&x_bfe_tjscore=0.100000&tngroupname=organic_news&newVideo=12&goods_entry_switch=1&pn=20\n",
      "10\n",
      "https://www.sohu.com/a/510098431_133140\n",
      "输入成功！\n",
      "https://news.sciencenet.cn/sbhtmlnews/2021/12/367036.shtm\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'gbk' codec can't encode character '\\xa0' in position 272: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-8e68929c9d9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;31m#key_word = input('input key word:')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m     \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"自然语言处理\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-8e68929c9d9d>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(key_word)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[1;31m#file.write(u'\\n' + contentlink + u'\\n')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m#file.close()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m                 \u001b[0mextract_news_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontentlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 还写入文件\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m                 \u001b[0mvisited_url_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontentlink\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 访问之\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[0mvisited_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./visited-cn.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 标记为已访问，永久存防止程序停止后丢失\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-8e68929c9d9d>\u001b[0m in \u001b[0;36mextract_news_content\u001b[1;34m(web_url, file_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mcontent_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# append\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"输入成功！\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'gbk' codec can't encode character '\\xa0' in position 272: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# 关键词是自然语言处理，能翻页\n",
    "\n",
    "import re\n",
    "import urllib.request\n",
    "import chardet\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "\n",
    "# 提取网页正文，放入txt中\n",
    "def remove_js_css(content):\n",
    "    \"\"\" remove the the javascript and the stylesheet and the comment content (<script>....</script> and <style>....</style> <!-- xxx -->) \"\"\"\n",
    "    content = content.decode('utf-8')\n",
    "    r = re.compile(r'''<script.*?</script>''', re.I | re.M | re.S)\n",
    "    s = r.sub('', content)\n",
    "    r = re.compile(r'''<style.*?</style>''', re.I | re.M | re.S)\n",
    "    s = r.sub('', s)\n",
    "    r = re.compile(r'''<!--.*?-->''', re.I | re.M | re.S)\n",
    "    s = r.sub('', s)\n",
    "    r = re.compile(r'''<meta.*?>''', re.I | re.M | re.S)\n",
    "    s = r.sub('', s)\n",
    "    r = re.compile(r'''<ins.*?</ins>''', re.I | re.M | re.S)\n",
    "    s = r.sub('', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def remove_empty_line(content):\n",
    "    \"\"\"remove multi space \"\"\"\n",
    "    r = re.compile(r'''^\\s+$''', re.M | re.S)\n",
    "    s = r.sub('', content)\n",
    "    r = re.compile(r'''\\n+''', re.M | re.S)\n",
    "    s = r.sub('\\n', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def remove_any_tag(s):\n",
    "    s = re.sub(r'''<[^>]+>''', '', s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def remove_any_tag_but_a(s):\n",
    "    text = re.findall(r'''<a[^r][^>]*>(.*?)</a>''', s, re.I | re.S | re.S)\n",
    "    text_b = remove_any_tag(s)\n",
    "    return len(''.join(text)), len(text_b)\n",
    "\n",
    "\n",
    "def remove_image(s, n=50):\n",
    "    image = 'a' * n\n",
    "    r = re.compile(r'''<img.*?>''', re.I | re.M | re.S)\n",
    "    s = r.sub(image, s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def remove_video(s, n=1000):\n",
    "    video = 'a' * n\n",
    "    r = re.compile(r'''<embed.*?>''', re.I | re.M | re.S)\n",
    "    s = r.sub(video, s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def sum_max(values):\n",
    "    cur_max = values[0]\n",
    "    glo_max = -999999\n",
    "    left, right = 0, 0\n",
    "    for index, value in enumerate(values):\n",
    "        cur_max += value\n",
    "        if (cur_max > glo_max):\n",
    "            glo_max = cur_max\n",
    "            right = index\n",
    "        elif (cur_max < 0):\n",
    "            cur_max = 0\n",
    "\n",
    "    for i in range(right, -1, -1):\n",
    "        glo_max -= values[i]\n",
    "        if abs(glo_max < 0.00001):\n",
    "            left = i\n",
    "            break\n",
    "    return left, right + 1\n",
    "\n",
    "\n",
    "def method_1(content, k=1):\n",
    "    if not content:\n",
    "        return None, None, None, None\n",
    "    tmp = content.split('\\n')\n",
    "    group_value = []\n",
    "    for i in range(0, len(tmp), k):\n",
    "        group = '\\n'.join(tmp[i:i + k])\n",
    "        group = remove_image(group)\n",
    "        group = remove_video(group)\n",
    "        text_a, text_b = remove_any_tag_but_a(group)\n",
    "        temp = (text_b - text_a) - 8\n",
    "        group_value.append(temp)\n",
    "    left, right = sum_max(group_value)\n",
    "    return left, right, len('\\n'.join(tmp[:left])), len('\\n'.join(tmp[:right]))\n",
    "\n",
    "\n",
    "def extract(content):\n",
    "    content = remove_empty_line(remove_js_css(content))\n",
    "    left, right, x, y = method_1(content)\n",
    "    return '\\n'.join(content.split('\\n')[left:right])\n",
    "\n",
    "\n",
    "# 输入url，将其新闻页的正文输入txt\n",
    "def extract_news_content(web_url, file_name):\n",
    "    headers = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36\"\n",
    "}\n",
    "    request = requests.get(web_url, headers=headers)\n",
    "    #opener = urllib.request.build_opener()\n",
    "    #html = opener.open(request).read()\n",
    "    html = request.content\n",
    "    infoencode = chardet.detect(html)['encoding']  # 提取网页的编码\n",
    "    if html != None and infoencode != None:  # 提取内容不为空，error.或者用else\n",
    "        html = html.decode(infoencode, 'ignore')\n",
    "        soup = bs(html,'lxml')\n",
    "        content = soup.renderContents()\n",
    "        content_text = extract(content)  # 提取新闻网页中的正文部分，化为无换行的一段文字\n",
    "        content_text = re.sub(\" \", \" \", content_text)\n",
    "        content_text = re.sub(\"&gt;\", \"\", content_text)\n",
    "        content_text = re.sub(\"&quot;\", '\"\"', content_text)\n",
    "        content_text = re.sub(\"<[^>]+>\", \"\", content_text)\n",
    "        content_text = re.sub(\"\\n\", \"\", content_text)\n",
    "        file = open(file_name, 'a')  # append\n",
    "        file.write(content_text)\n",
    "        file.close()\n",
    "        print(\"输入成功！\")\n",
    "\n",
    "\n",
    "# 抓取百度新闻搜索结果:中文搜索，前10页，url：key=关键词\n",
    "def search(key_word):\n",
    "    headers = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36\"\n",
    "}\n",
    "    search_url = 'http://news.baidu.com/ns?word=key_word&tn=news&from=news&cl=2&rn=20&ct=1'\n",
    "    search_url = search_url.replace('key_word',key_word)\n",
    "    print(search_url)\n",
    "    req = requests.get(search_url, headers=headers)\n",
    "    #print(req.text)\n",
    "    real_visited = 0\n",
    "    http_url=None\n",
    "    url = \"http://www.baidu.com\"\n",
    "    visited_url_list = []\n",
    "    for count in range(2):  # 前2页\n",
    "        if count>0:\n",
    "            req=requests.get(http_url)\n",
    "        html = req.content\n",
    "        soup = bs(html, 'lxml')\n",
    "        content = soup.findAll( \"div\", {\"class\":\"result-op\"})  # resultset object\n",
    "        #print(content)\n",
    "        next=soup.findAll(\"a\",{\"class\":\"n\"})\n",
    "        #print(next)\n",
    "        dddd=len(next) - 1\n",
    "        if dddd<0:\n",
    "            print(\"未搜索到\")\n",
    "            return\n",
    "        ssss=next[dddd].get('href')\n",
    "        http_url=url+ssss\n",
    "        print(http_url)\n",
    "        num = len(content)\n",
    "        print(str(num))\n",
    "        for i in range(num):\n",
    "            p_str = content[i].find('a')  # if no result then nontype object\n",
    "            contenttitle = p_str.renderContents()\n",
    "            contenttitle = contenttitle.decode('utf-8', 'ignore')  # need it\n",
    "            contenttitle = re.sub(\"<[^>]+>\", \"\", contenttitle)\n",
    "            contentlink = str(p_str.get(\"href\"))\n",
    "            print(contentlink)\n",
    "            # 存放顺利抓取的url，对比\n",
    "            visited_url = open('./visited.txt', 'r')  # 是否已经爬过\n",
    "            visited_url_list = visited_url.readlines()\n",
    "            visited_url.close()  # 及时close\n",
    "            exist = 0\n",
    "            if contentlink in visited_url_list:\n",
    "                exist = 1\n",
    "            else:  # 如果未被访问url\n",
    "                real_visited += 1\n",
    "                file_name = './newscn/%d.txt' % (real_visited)\n",
    "                extract_news_content(contentlink, file_name)  # 写入文件\n",
    "                visited_url_list.append(contentlink)\n",
    "                visited_url = open('./visited.txt', 'a')  # 标记为已访问，永久存防止程序停止后丢失\n",
    "                visited_url.write(contentlink + u'\\n')\n",
    "                visited_url.close()\n",
    "            if len(visited_url_list) >= 120:\n",
    "                print(\"解析下一页\")\n",
    "        if len(visited_url_list) >= 120:\n",
    "            print(\"解析下一页\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #key_word = input('input key word:')\n",
    "    search(\"自然语言处理\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./newscn/1.txt', './newscn/2.txt', './newscn/3.txt']\n",
      "./newscn/1.txt\n",
      "鸥玛软件:公司与山东大学合作在自然语言处理、机器学习、智能识别...同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。这些应也是元宇宙科技创新的基本...百度快照鸥玛软件:公司与山东大学合作在自然语言处理、机器学习、智能识别...同花顺财经昨天17:03同花顺金融研究中心12月23日讯,有投资者向鸥玛软件提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。这些应也是元宇宙科技创新的基本步骤?谢谢尊敬的董...百度快照从愿景到现实:人工智能在医疗保健领域的兴起 | 元宇宙网易新闻昨天12:17AI 的核心是机器学习,它由三个认知节点组成:计算机视觉、自然语言处理和数据推理。计算机视觉是人工智能的“眼睛”,因为它能够比人类更快地识别数字图像中的视觉模式、对象、场景和活动。自然语言处理是指识别和理解口语的技术。结构化数据...百度快照...在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等...证券之星昨天16:29公司与山东大学通过共建“山东大学-鸥玛软件人工智能创新研究院”, 在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合攻关研究、产业化应用、人才培养等方面进行深度合作。感谢您的关注与支持!百度快照科学家开发新机器学习模型,让计算机“读懂”人类对话网易前天16:15在研究和分析人与人对话的时候,研究人员发现许多自然语言处理算法,只有在文本具有清晰结构时才能正常运行,例如当一个人用完整的句子说话的时候。然而,在现实生活中,人们很少说话如此正式,这使得系统很难准确确定句子的开头和结尾。 百度快照百度首席技术官王海峰:深耕自然语言处理近30年,推进AI融合创新人民资讯12月13日该技术旨在突破跨模态语义理解瓶颈，让机器能够像人一样，通过语言、听觉、视觉等获得对真实世界的统一认知，实现对复杂场景的理解。作为此项技术第一完成人，王海峰在《对话》节目中谈到，自然语言处理是人工智能领域的重要方向，他已研究...百度快照...基于预训练的语义排序|自然语言处理|知识产权|语义_新浪新闻新浪新闻3天前本项目由智慧芽投递并参与“数据猿年度金猿策划活动——2021大数据产业创新技术突破榜榜单及奖项”评选。 通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分...百度快照恒州博智|中国自然语言处理软件市场现状及未来发展趋势恒州诚思QYResearch9天前本文研究中国市场自然语言处理软件现状及未来发展趋势，侧重分析在中国市场扮演重要角色的企业，重点呈现这些企业在中国市场的自然语言处理软件收入、市场份额、市场定位、发展计划、产品及服务等。历史数据为2016至2020年，预测数据为2021至2027...百度快照【金猿技术展】专利智能语义检索——基于预训练的语义排序新浪新闻前天12:27通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分析与文本挖掘,进一步实现深层次语义分析,为用户提供更加精准地语义检索服务。 从不同的数据源入手,智慧芽...百度快照容联云AI商业大会:在商业场景下 自然语言处理如何联动决策智能...驱动之家12月13日在前不久容联云举办的 「AI有心 决策有智」的AI商业大会上，AI科技评论对话到容联云AI科学院院长刘杰，讨论了认知智能与沟通技术的演进，以及在商业场景下，自然语言处理如何联动决策智能共同发展。在营销服务的沟通场景下，Al策略是否只...百度快照鸥玛软件:公司与山东大学合作在自然语言处理、机器学习、智能识别...同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。这些应也是元宇宙科技创新的基本...百度快照鸥玛软件:公司与山东大学合作在自然语言处理、机器学习、智能识别...同花顺财经昨天17:03同花顺金融研究中心12月23日讯,有投资者向鸥玛软件提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。这些应也是元宇宙科技创新的基本步骤?谢谢尊敬的董...百度快照从愿景到现实:人工智能在医疗保健领域的兴起 | 元宇宙网易新闻昨天12:17AI 的核心是机器学习,它由三个认知节点组成:计算机视觉、自然语言处理和数据推理。计算机视觉是人工智能的“眼睛”,因为它能够比人类更快地识别数字图像中的视觉模式、对象、场景和活动。自然语言处理是指识别和理解口语的技术。结构化数据...百度快照...在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等...证券之星昨天16:29公司与山东大学通过共建“山东大学-鸥玛软件人工智能创新研究院”, 在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合攻关研究、产业化应用、人才培养等方面进行深度合作。感谢您的关注与支持!百度快照科学家开发新机器学习模型,让计算机“读懂”人类对话网易前天16:15在研究和分析人与人对话的时候,研究人员发现许多自然语言处理算法,只有在文本具有清晰结构时才能正常运行,例如当一个人用完整的句子说话的时候。然而,在现实生活中,人们很少说话如此正式,这使得系统很难准确确定句子的开头和结尾。 百度快照百度首席技术官王海峰:深耕自然语言处理近30年,推进AI融合创新人民资讯12月13日该技术旨在突破跨模态语义理解瓶颈，让机器能够像人一样，通过语言、听觉、视觉等获得对真实世界的统一认知，实现对复杂场景的理解。作为此项技术第一完成人，王海峰在《对话》节目中谈到，自然语言处理是人工智能领域的重要方向，他已研究...百度快照...基于预训练的语义排序|自然语言处理|知识产权|语义_新浪新闻新浪新闻3天前本项目由智慧芽投递并参与“数据猿年度金猿策划活动——2021大数据产业创新技术突破榜榜单及奖项”评选。 通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分...百度快照恒州博智|中国自然语言处理软件市场现状及未来发展趋势恒州诚思QYResearch9天前本文研究中国市场自然语言处理软件现状及未来发展趋势，侧重分析在中国市场扮演重要角色的企业，重点呈现这些企业在中国市场的自然语言处理软件收入、市场份额、市场定位、发展计划、产品及服务等。历史数据为2016至2020年，预测数据为2021至2027...百度快照【金猿技术展】专利智能语义检索——基于预训练的语义排序新浪新闻前天12:27通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分析与文本挖掘,进一步实现深层次语义分析,为用户提供更加精准地语义检索服务。 从不同的数据源入手,智慧芽...百度快照容联云AI商业大会:在商业场景下 自然语言处理如何联动决策智能...驱动之家12月13日在前不久容联云举办的 「AI有心 决策有智」的AI商业大会上，AI科技评论对话到容联云AI科学院院长刘杰，讨论了认知智能与沟通技术的演进，以及在商业场景下，自然语言处理如何联动决策智能共同发展。在营销服务的沟通场景下，Al策略是否只...百度快照\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺金融研究中心12月23日讯,有投资者向鸥玛软件提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺金融研究中心12月23日讯,有投资者向鸥玛软件提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。百度快照【金猿技术展】专利智能语义检索——基于预训练的语义排序新浪新闻前天12:27通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分析与文本挖掘,进一步实现深层次语义分析,为用户提供更加精准地语义检索服务。\n",
      "\n",
      "\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。百度快照\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺金融研究中心12月23日讯,有投资者向鸥玛软件提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "\n",
      "\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。百度快照\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "\n",
      "\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。百度快照\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。鸥玛软件:公司与山东大学合作在自然语言处理、机器学习、智能识别...同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。这些应也是元宇宙科技创新的基本...百度快照鸥玛软件:公司与山东大学合作在自然语言处理、机器学习、智能识别...同花顺财经昨天17:03同花顺金融研究中心12月23日讯,有投资者向鸥玛软件提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。这些应也是元宇宙科技创新的基本步骤?谢谢尊敬的董...百度快照从愿景到现实:人工智能在医疗保健领域的兴起 | 元宇宙网易新闻昨天12:17AI 的核心是机器学习,它由三个认知节点组成:计算机视觉、自然语言处理和数据推理。计算机视觉是人工智能的“眼睛”,因为它能够比人类更快地识别数字图像中的视觉模式、对象、场景和活动。自然语言处理是指识别和理解口语的技术。结构化数据...百度快照...鸥玛软件人工智能创新研究院”,在自然语言处理、机器学习...证券之星昨天16:49公司与山东大学通过共建“山东大学-鸥玛软件人工智能创新研究院”,在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合攻关研究、产业化应用、人才培养等方面进行深度合作。感谢您的关注与支持!百度快照科学家开发新机器学习模型,让计算机“读懂”人类对话网易前天16:15在研究和分析人与人对话的时候,研究人员发现许多自然语言处理算法,只有在文本具有清晰结构时才能正常运行,例如当一个人用完整的句子说话的时候。然而,在现实生活中,人们很少说话如此正式,这使得系统很难准确确定句子的开头和结尾。 百度快照百度首席技术官王海峰:深耕自然语言处理近30年,推进AI融合创新人民资讯12月13日该技术旨在突破跨模态语义理解瓶颈，让机器能够像人一样，通过语言、听觉、视觉等获得对真实世界的统一认知，实现对复杂场景的理解。作为此项技术第一完成人，王海峰在《对话》节目中谈到，自然语言处理是人工智能领域的重要方向，他已研究...百度快照...基于预训练的语义排序|自然语言处理|知识产权|语义_新浪新闻新浪新闻3天前本项目由智慧芽投递并参与“数据猿年度金猿策划活动——2021大数据产业创新技术突破榜榜单及奖项”评选。 通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分...百度快照恒州博智|中国自然语言处理软件市场现状及未来发展趋势恒州诚思QYResearch9天前本文研究中国市场自然语言处理软件现状及未来发展趋势，侧重分析在中国市场扮演重要角色的企业，重点呈现这些企业在中国市场的自然语言处理软件收入、市场份额、市场定位、发展计划、产品及服务等。历史数据为2016至2020年，预测数据为2021至2027...百度快照【金猿技术展】专利智能语义检索——基于预训练的语义排序新浪新闻前天12:27通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分析与文本挖掘,进一步实现深层次语义分析,为用户提供更加精准地语义检索服务。 从不同的数据源入手,智慧芽...百度快照容联云AI商业大会:在商业场景下 自然语言处理如何联动决策智能...驱动之家12月13日在前不久容联云举办的 「AI有心 决策有智」的AI商业大会上，AI科技评论对话到容联云AI科学院院长刘杰，讨论了认知智能与沟通技术的演进，以及在商业场景下，自然语言处理如何联动决策智能共同发展。在营销服务的沟通场景下，Al策略是否只...百度快照\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。百度快照\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。百度快照\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。百度快照\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。百度快照\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。 扫描添加 同花顺财经官方微信号 扫描添加 手机同花顺财经 扫描添加 专业炒股利器 扫描添加 同花顺爱基金投资者关系 | 关于同花顺 | 软件下载 | 法律声明 | 运营许可 | 联系我们 | 友情链接 | 招聘英才 | 用户体验计划          |               涉未成年人违规内容举报     不良信息举报电话：(0571)88933003 \t\t\t举报邮箱：jubao@myhexin.com \t\t\t增值电信业务经营许可证：B2-20090237Copyright Zhejiang RoyalFlush Network Technology Co., Ltd. All rights reserved. 浙江同花顺网络科技有限公司版权所有网站备案号： \t\t\t浙ICP备10056399号-3 \t\t\t证券投资咨询服务提供：浙江同花顺云软件有限公司 （中国证监会核发证书编号：ZX0050） 不良信息举报 浙公网安备 33011002011820号\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。百度快照\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。百度快照\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "-----------approach 1-------------\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "百度快照\n",
      "\n",
      "摘要：\n",
      "\n",
      "\n",
      "摘要：\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "./newscn/2.txt\n",
      "鸥玛软件:公司与山东大学合作在自然语言处理、机器学习、智能识别...同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。这些应也是元宇宙科技创新的基本...百度快照鸥玛软件:公司与山东大学合作在自然语言处理、机器学习、智能识别...同花顺财经昨天17:03同花顺金融研究中心12月23日讯,有投资者向鸥玛软件提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。这些应也是元宇宙科技创新的基本步骤?谢谢尊敬的董...百度快照从愿景到现实:人工智能在医疗保健领域的兴起 | 元宇宙网易新闻昨天12:17AI 的核心是机器学习,它由三个认知节点组成:计算机视觉、自然语言处理和数据推理。计算机视觉是人工智能的“眼睛”,因为它能够比人类更快地识别数字图像中的视觉模式、对象、场景和活动。自然语言处理是指识别和理解口语的技术。结构化数据...百度快照...在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等...证券之星昨天16:29公司与山东大学通过共建“山东大学-鸥玛软件人工智能创新研究院”, 在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合攻关研究、产业化应用、人才培养等方面进行深度合作。感谢您的关注与支持!百度快照科学家开发新机器学习模型,让计算机“读懂”人类对话网易前天16:15在研究和分析人与人对话的时候,研究人员发现许多自然语言处理算法,只有在文本具有清晰结构时才能正常运行,例如当一个人用完整的句子说话的时候。然而,在现实生活中,人们很少说话如此正式,这使得系统很难准确确定句子的开头和结尾。 百度快照百度首席技术官王海峰:深耕自然语言处理近30年,推进AI融合创新人民资讯12月13日该技术旨在突破跨模态语义理解瓶颈，让机器能够像人一样，通过语言、听觉、视觉等获得对真实世界的统一认知，实现对复杂场景的理解。作为此项技术第一完成人，王海峰在《对话》节目中谈到，自然语言处理是人工智能领域的重要方向，他已研究...百度快照...基于预训练的语义排序|自然语言处理|知识产权|语义_新浪新闻新浪新闻3天前本项目由智慧芽投递并参与“数据猿年度金猿策划活动——2021大数据产业创新技术突破榜榜单及奖项”评选。 通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分...百度快照恒州博智|中国自然语言处理软件市场现状及未来发展趋势恒州诚思QYResearch9天前本文研究中国市场自然语言处理软件现状及未来发展趋势，侧重分析在中国市场扮演重要角色的企业，重点呈现这些企业在中国市场的自然语言处理软件收入、市场份额、市场定位、发展计划、产品及服务等。历史数据为2016至2020年，预测数据为2021至2027...百度快照【金猿技术展】专利智能语义检索——基于预训练的语义排序新浪新闻前天12:27通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分析与文本挖掘,进一步实现深层次语义分析,为用户提供更加精准地语义检索服务。 从不同的数据源入手,智慧芽...百度快照容联云AI商业大会:在商业场景下 自然语言处理如何联动决策智能...驱动之家12月13日在前不久容联云举办的 「AI有心 决策有智」的AI商业大会上，AI科技评论对话到容联云AI科学院院长刘杰，讨论了认知智能与沟通技术的演进，以及在商业场景下，自然语言处理如何联动决策智能共同发展。在营销服务的沟通场景下，Al策略是否只...百度快照鸥玛软件:公司与山东大学合作在自然语言处理、机器学习、智能识别...同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。这些应也是元宇宙科技创新的基本...百度快照鸥玛软件:公司与山东大学合作在自然语言处理、机器学习、智能识别...同花顺财经昨天17:03同花顺金融研究中心12月23日讯,有投资者向鸥玛软件提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。这些应也是元宇宙科技创新的基本步骤?谢谢尊敬的董...百度快照从愿景到现实:人工智能在医疗保健领域的兴起 | 元宇宙网易新闻昨天12:17AI 的核心是机器学习,它由三个认知节点组成:计算机视觉、自然语言处理和数据推理。计算机视觉是人工智能的“眼睛”,因为它能够比人类更快地识别数字图像中的视觉模式、对象、场景和活动。自然语言处理是指识别和理解口语的技术。结构化数据...百度快照...鸥玛软件人工智能创新研究院”,在自然语言处理、机器学习...证券之星昨天16:49公司与山东大学通过共建“山东大学-鸥玛软件人工智能创新研究院”,在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合攻关研究、产业化应用、人才培养等方面进行深度合作。感谢您的关注与支持!百度快照科学家开发新机器学习模型,让计算机“读懂”人类对话网易前天16:15在研究和分析人与人对话的时候,研究人员发现许多自然语言处理算法,只有在文本具有清晰结构时才能正常运行,例如当一个人用完整的句子说话的时候。然而,在现实生活中,人们很少说话如此正式,这使得系统很难准确确定句子的开头和结尾。 百度快照百度首席技术官王海峰:深耕自然语言处理近30年,推进AI融合创新人民资讯12月13日该技术旨在突破跨模态语义理解瓶颈，让机器能够像人一样，通过语言、听觉、视觉等获得对真实世界的统一认知，实现对复杂场景的理解。作为此项技术第一完成人，王海峰在《对话》节目中谈到，自然语言处理是人工智能领域的重要方向，他已研究...百度快照...基于预训练的语义排序|自然语言处理|知识产权|语义_新浪新闻新浪新闻3天前本项目由智慧芽投递并参与“数据猿年度金猿策划活动——2021大数据产业创新技术突破榜榜单及奖项”评选。 通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分...百度快照恒州博智|中国自然语言处理软件市场现状及未来发展趋势恒州诚思QYResearch9天前本文研究中国市场自然语言处理软件现状及未来发展趋势，侧重分析在中国市场扮演重要角色的企业，重点呈现这些企业在中国市场的自然语言处理软件收入、市场份额、市场定位、发展计划、产品及服务等。历史数据为2016至2020年，预测数据为2021至2027...百度快照【金猿技术展】专利智能语义检索——基于预训练的语义排序新浪新闻前天12:27通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分析与文本挖掘,进一步实现深层次语义分析,为用户提供更加精准地语义检索服务。 从不同的数据源入手,智慧芽...百度快照容联云AI商业大会:在商业场景下 自然语言处理如何联动决策智能...驱动之家12月13日在前不久容联云举办的 「AI有心 决策有智」的AI商业大会上，AI科技评论对话到容联云AI科学院院长刘杰，讨论了认知智能与沟通技术的演进，以及在商业场景下，自然语言处理如何联动决策智能共同发展。在营销服务的沟通场景下，Al策略是否只...百度快照 扫描添加 同花顺财经官方微信号 扫描添加 手机同花顺财经 扫描添加 专业炒股利器 扫描添加 同花顺爱基金投资者关系 | 关于同花顺 | 软件下载 | 法律声明 | 运营许可 | 联系我们 | 友情链接 | 招聘英才 | 用户体验计划          |               涉未成年人违规内容举报     不良信息举报电话：(0571)88933003 \t\t\t举报邮箱：jubao@myhexin.com \t\t\t增值电信业务经营许可证：B2-20090237Copyright Zhejiang RoyalFlush Network Technology Co., Ltd. All rights reserved. 浙江同花顺网络科技有限公司版权所有网站备案号： \t\t\t浙ICP备10056399号-3 \t\t\t证券投资咨询服务提供：浙江同花顺云软件有限公司 （中国证监会核发证书编号：ZX0050） 不良信息举报 浙公网安备 33011002011820号\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------approach 1-------------\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "同花顺财经昨天17:03同花顺金融研究中心12月23日讯,有投资者向鸥玛软件提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "同花顺财经昨天17:03同花顺金融研究中心12月23日讯,有投资者向鸥玛软件提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "百度快照【金猿技术展】专利智能语义检索——基于预训练的语义排序新浪新闻前天12:27通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分析与文本挖掘,进一步实现深层次语义分析,为用户提供更加精准地语义检索服务。\n",
      "./newscn/3.txt\n",
      "鸥玛软件:公司与山东大学合作在自然语言处理、机器学习、智能识别...同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。这些应也是元宇宙科技创新的基本...百度快照鸥玛软件:公司与山东大学合作在自然语言处理、机器学习、智能识别...同花顺财经昨天17:03同花顺金融研究中心12月23日讯,有投资者向鸥玛软件提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。这些应也是元宇宙科技创新的基本步骤?谢谢尊敬的董...百度快照从愿景到现实:人工智能在医疗保健领域的兴起 | 元宇宙网易新闻昨天12:17AI 的核心是机器学习,它由三个认知节点组成:计算机视觉、自然语言处理和数据推理。计算机视觉是人工智能的“眼睛”,因为它能够比人类更快地识别数字图像中的视觉模式、对象、场景和活动。自然语言处理是指识别和理解口语的技术。结构化数据...百度快照...在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等...证券之星昨天16:29公司与山东大学通过共建“山东大学-鸥玛软件人工智能创新研究院”, 在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合攻关研究、产业化应用、人才培养等方面进行深度合作。感谢您的关注与支持!百度快照科学家开发新机器学习模型,让计算机“读懂”人类对话网易前天16:15在研究和分析人与人对话的时候,研究人员发现许多自然语言处理算法,只有在文本具有清晰结构时才能正常运行,例如当一个人用完整的句子说话的时候。然而,在现实生活中,人们很少说话如此正式,这使得系统很难准确确定句子的开头和结尾。 百度快照百度首席技术官王海峰:深耕自然语言处理近30年,推进AI融合创新人民资讯12月13日该技术旨在突破跨模态语义理解瓶颈，让机器能够像人一样，通过语言、听觉、视觉等获得对真实世界的统一认知，实现对复杂场景的理解。作为此项技术第一完成人，王海峰在《对话》节目中谈到，自然语言处理是人工智能领域的重要方向，他已研究...百度快照...基于预训练的语义排序|自然语言处理|知识产权|语义_新浪新闻新浪新闻3天前本项目由智慧芽投递并参与“数据猿年度金猿策划活动——2021大数据产业创新技术突破榜榜单及奖项”评选。 通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分...百度快照恒州博智|中国自然语言处理软件市场现状及未来发展趋势恒州诚思QYResearch9天前本文研究中国市场自然语言处理软件现状及未来发展趋势，侧重分析在中国市场扮演重要角色的企业，重点呈现这些企业在中国市场的自然语言处理软件收入、市场份额、市场定位、发展计划、产品及服务等。历史数据为2016至2020年，预测数据为2021至2027...百度快照【金猿技术展】专利智能语义检索——基于预训练的语义排序新浪新闻前天12:27通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分析与文本挖掘,进一步实现深层次语义分析,为用户提供更加精准地语义检索服务。 从不同的数据源入手,智慧芽...百度快照容联云AI商业大会:在商业场景下 自然语言处理如何联动决策智能...驱动之家12月13日在前不久容联云举办的 「AI有心 决策有智」的AI商业大会上，AI科技评论对话到容联云AI科学院院长刘杰，讨论了认知智能与沟通技术的演进，以及在商业场景下，自然语言处理如何联动决策智能共同发展。在营销服务的沟通场景下，Al策略是否只...百度快照鸥玛软件:公司与山东大学合作在自然语言处理、机器学习、智能识别...同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。这些应也是元宇宙科技创新的基本...百度快照鸥玛软件:公司与山东大学合作在自然语言处理、机器学习、智能识别...同花顺财经昨天17:03同花顺金融研究中心12月23日讯,有投资者向鸥玛软件提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。这些应也是元宇宙科技创新的基本步骤?谢谢尊敬的董...百度快照从愿景到现实:人工智能在医疗保健领域的兴起 | 元宇宙网易新闻昨天12:17AI 的核心是机器学习,它由三个认知节点组成:计算机视觉、自然语言处理和数据推理。计算机视觉是人工智能的“眼睛”,因为它能够比人类更快地识别数字图像中的视觉模式、对象、场景和活动。自然语言处理是指识别和理解口语的技术。结构化数据...百度快照...鸥玛软件人工智能创新研究院”,在自然语言处理、机器学习...证券之星昨天16:49公司与山东大学通过共建“山东大学-鸥玛软件人工智能创新研究院”,在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合攻关研究、产业化应用、人才培养等方面进行深度合作。感谢您的关注与支持!百度快照科学家开发新机器学习模型,让计算机“读懂”人类对话网易前天16:15在研究和分析人与人对话的时候,研究人员发现许多自然语言处理算法,只有在文本具有清晰结构时才能正常运行,例如当一个人用完整的句子说话的时候。然而,在现实生活中,人们很少说话如此正式,这使得系统很难准确确定句子的开头和结尾。 百度快照百度首席技术官王海峰:深耕自然语言处理近30年,推进AI融合创新人民资讯12月13日该技术旨在突破跨模态语义理解瓶颈，让机器能够像人一样，通过语言、听觉、视觉等获得对真实世界的统一认知，实现对复杂场景的理解。作为此项技术第一完成人，王海峰在《对话》节目中谈到，自然语言处理是人工智能领域的重要方向，他已研究...百度快照...基于预训练的语义排序|自然语言处理|知识产权|语义_新浪新闻新浪新闻3天前本项目由智慧芽投递并参与“数据猿年度金猿策划活动——2021大数据产业创新技术突破榜榜单及奖项”评选。 通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分...百度快照恒州博智|中国自然语言处理软件市场现状及未来发展趋势恒州诚思QYResearch9天前本文研究中国市场自然语言处理软件现状及未来发展趋势，侧重分析在中国市场扮演重要角色的企业，重点呈现这些企业在中国市场的自然语言处理软件收入、市场份额、市场定位、发展计划、产品及服务等。历史数据为2016至2020年，预测数据为2021至2027...百度快照【金猿技术展】专利智能语义检索——基于预训练的语义排序新浪新闻前天12:27通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分析与文本挖掘,进一步实现深层次语义分析,为用户提供更加精准地语义检索服务。 从不同的数据源入手,智慧芽...百度快照容联云AI商业大会:在商业场景下 自然语言处理如何联动决策智能...驱动之家12月13日在前不久容联云举办的 「AI有心 决策有智」的AI商业大会上，AI科技评论对话到容联云AI科学院院长刘杰，讨论了认知智能与沟通技术的演进，以及在商业场景下，自然语言处理如何联动决策智能共同发展。在营销服务的沟通场景下，Al策略是否只...百度快照近日，约翰霍普金斯大学语言和语音处理中心（Johns Hopkins Center for Language and Speech Processing）的工程师开发了一种新机器学习模型，该模型可以进行语言理解，或区分LU系统输出的对话记录中的语音功能，这种方法最终可以帮助计算机“理解”口语或语音，并书写成通顺文本。这个新开发的模型，能够识别词语背后的意图，并将它们组织与分类成最终的“陈述”、“问题”或 “打断”等类别，这项任务被称为\"对话行为的识别\"。研究人员表示，通过为其它模型提供一个更有组织和细分的文本版本，新模型可以成为理解对话的第一个步骤。这种新方法意味着，LU系统不再需要处理巨大的、非结构化的文本块，它们在试图对文本的主题、情感或意图等进行分类时，会遇到困难。相反，他们可以处理一系列的表达，这些表达说的是非常具体的事情，如一个问题或中断。研究中，研究人员采用了一些最近引入的语言理解模型，目标是组织和分类单词和短语，并研究不同的变量（例如标点符号）如何影响这些模型的性能。研究者称，标点符号为模型提供了非常强的线索，而这些线索在文本中似乎并不存在，例如对话中几乎找不到标点符号。在研究和分析人与人对话的时候，研究人员发现许多自然语言处理算法，只有在文本具有清晰结构时才能正常运行，例如当一个人用完整的句子说话的时候。然而，在现实生活中，人们很少说话如此正式，这使得系统很难准确确定句子的开头和结尾。但是有了这个新模型，至少可以找到对话的“单元”。这可能有助于完成大量任务，例如总结、意图识别和关键短语的检测。研究人员预测，这个新模型有一天也可以被医生使用，为他们节省宝贵的时间。医生如今在与患者互动时，会花时间做笔记。然而，使用新模型的设备可以快速浏览对话记录、填写表格并自动写笔记，让医生能够专注于他们患者的其它情况。题为What Helps Transformers Recognize Conversational Structure? Importance of Context, Punctuation, and Labels in Dialog Act Recognition的相关研究论文发表在《计算语言学协会学报》（Transactions of the Association for Computational Linguistics）上。前瞻经济学人APP资讯组论文原文：https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00420/107831/What-Helps-Transformers-Recognize-Conversational特别声明：以上内容(如有图片或视频亦包括在内)为自媒体平台“网易号”用户上传并发布，本平台仅提供信息存储服务。Notice: The content above (including the pictures and videos if any) is uploaded and posted by a user of NetEase Hao, which is a social media platform and only provides information storage services.\n",
      "-----------approach 1-------------\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "同花顺财经昨天17:03同花顺金融研究中心12月23日讯,有投资者向鸥玛软件提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "同花顺财经昨天17:03同花顺(300033)金融研究中心12月23日讯,有投资者向鸥玛软件(301185)提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "同花顺财经昨天17:03同花顺金融研究中心12月23日讯,有投资者向鸥玛软件提问, 从新闻公告中得知公司在自然语言处理、机器学习、智能识别、人机交互及虚拟现实等人工智能技术方面联合山东大学攻关研究、深度合作。\n",
      "百度快照【金猿技术展】专利智能语义检索——基于预训练的语义排序新浪新闻前天12:27通过深度学习、自然语言处理以及预训练语言模型等前沿人工智能技术的运用,实现在海量全球多语言专利文本中进行自动化、智能化的数据分析与文本挖掘,进一步实现深层次语义分析,为用户提供更加精准地语义检索服务。\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# 文本摘要方法有很多，主要分为抽取式和生成式，应用比较多的是抽取式，也比较简单，就是从文本中抽取重要的句子或段落。本方法主要是利用句子中的关键词的距离，主要思想和参考来自阮一峰的网络日志http://www.ruanyifeng.com/blog/2013/03/automatic_summarization.html\n",
    "import nltk\n",
    "import numpy\n",
    "import jieba\n",
    "import codecs\n",
    "# N=100#单词数量\n",
    "# CLUSTER_THRESHOLD=5#单词间的距离\n",
    "# TOP_SENTENCES=5#返回的top n句子\n",
    "\n",
    "#分句\n",
    "def sent_tokenizer(texts):\n",
    "    start=0\n",
    "    i=0#每个字符的位置\n",
    "    sentences=[]\n",
    "    punt_list='.!?。！？' #',.!?:;~，。！？：；～'.decode('utf8')\n",
    "    token=\"\"\n",
    "    for text in texts:\n",
    "        if text in punt_list and token not in punt_list: #检查标点符号下一个字符是否还是标点\n",
    "            sentences.append(texts[start:i+1])#当前标点符号位置\n",
    "            start=i+1#start标记到下一句的开头\n",
    "            i+=1\n",
    "        else:\n",
    "            i+=1#若不是标点符号，则字符位置继续前移\n",
    "            token=list(texts[start:i+2]).pop()#取下一个字符\n",
    "    if start<len(texts):\n",
    "        sentences.append(texts[start:])#这是为了处理文本末尾没有标点符号的情况\n",
    "    return sentences\n",
    "\n",
    "#摘要\n",
    "def summarize(text):\n",
    "    N = 100\n",
    "    TOP_SENTENCES = 5\n",
    "    sentences=sent_tokenizer(text)\n",
    "    words=[w for sentence in sentences for w in jieba.cut(sentence) if len(w)>1 and w!='\\t']\n",
    "    wordfre=nltk.FreqDist(words)\n",
    "    topn_words=[w[0] for w in sorted(wordfre.items(),key=lambda d:d[1],reverse=True)][:N]\n",
    "    scored_sentences=_score_sentences(sentences,topn_words)\n",
    "    #approach 1,利用均值和标准差过滤非重要句子\n",
    "    avg=numpy.mean([s[1] for s in scored_sentences])#均值\n",
    "    std=numpy.std([s[1] for s in scored_sentences])#标准差\n",
    "    mean_scored=[(sent_idx,score) for (sent_idx,score) in scored_sentences if score>(avg+0.5*std)]\n",
    "    #approach 2，返回top n句子\n",
    "    top_n_scored=sorted(scored_sentences,key=lambda s:s[1])[-TOP_SENTENCES:]\n",
    "    top_n_scored=sorted(top_n_scored,key=lambda s:s[0])\n",
    "    top_n_summary = [sentences[idx] for (idx, score) in top_n_scored]\n",
    "    mean_scored_summary = [sentences[idx] for (idx, score) in mean_scored]\n",
    "    di={\"top_n_summary\":top_n_summary, \"mean_scored_summary\":mean_scored_summary}\n",
    "    return di\n",
    "\n",
    " #句子得分\n",
    "def _score_sentences(sentences,topn_words):\n",
    "    CLUSTER_THRESHOLD = 5\n",
    "    scores=[]\n",
    "    sentence_idx=-1\n",
    "    for s in [list(jieba.cut(s)) for s in sentences]:\n",
    "        sentence_idx+=1\n",
    "        word_idx=[]\n",
    "        for w in topn_words:\n",
    "            try:\n",
    "                word_idx.append(s.index(w))#关键词出现在该句子中的索引位置\n",
    "            except ValueError:#w不在句子中\n",
    "                pass\n",
    "        word_idx.sort()\n",
    "        if len(word_idx)==0:\n",
    "            continue\n",
    "        #对于两个连续的单词，利用单词位置索引，通过距离阀值计算簇\n",
    "        clusters=[]\n",
    "        cluster=[word_idx[0]]\n",
    "        i=1\n",
    "        while i<len(word_idx):\n",
    "            if word_idx[i]-word_idx[i-1]<CLUSTER_THRESHOLD:\n",
    "                cluster.append(word_idx[i])\n",
    "            else:\n",
    "                clusters.append(cluster[:])\n",
    "                cluster=[word_idx[i]]\n",
    "            i+=1\n",
    "        clusters.append(cluster)\n",
    "        #对每个族打分，每个族类的最大分数是对句子的打分\n",
    "        max_cluster_score=0\n",
    "        for c in clusters:\n",
    "            significant_words_in_cluster=len(c)\n",
    "            total_words_in_cluster=c[-1]-c[0]+1\n",
    "            score=1.0*significant_words_in_cluster*significant_words_in_cluster/total_words_in_cluster\n",
    "            if score>max_cluster_score:\n",
    "                max_cluster_score=score\n",
    "        scores.append((sentence_idx,max_cluster_score))\n",
    "    return scores;\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    text = []\n",
    "    for i in range(3):\n",
    "        filename = './newscn/%d.txt' % (i+1)\n",
    "        file = text.append(filename)\n",
    "        # print filename\n",
    "    #\n",
    "    print(text)\n",
    "    for item in text:\n",
    "        print(item)\n",
    "        file_ = open(item).read()\n",
    "        print(file_)\n",
    "        dict = summarize(file_)\n",
    "        file = open(item, 'a+')\n",
    "        file.write(\"\\n\\n摘要：\\n\")\n",
    "        print('-----------approach 1-------------')\n",
    "        for sent in dict['top_n_summary']:\n",
    "            file.write(sent)\n",
    "            print(sent)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "        #print('-----------approach 2-------------')\n",
    "        #for sent in dict['mean_scored_summary']:\n",
    "        #    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./newscn/1.txt\n",
      "./newscn/2.txt\n",
      "./newscn/3.txt\n",
      "./newscn/1.txt\n",
      "112\n",
      "./newscn/2.txt\n",
      "43\n",
      "./newscn/3.txt\n",
      "44\n",
      "[{'file_dir': './newscn/1.txt', 'freq': 112}, {'file_dir': './newscn/3.txt', 'freq': 44}, {'file_dir': './newscn/2.txt', 'freq': 43}]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def getvalue(direct):\n",
    "    return direct[\"freq\"]\n",
    "a = []\n",
    "for i in range(1,4):\n",
    "    filename = './newscn/%d.txt' % (i)\n",
    "    file = a.append(filename)\n",
    "    print(filename)\n",
    "\n",
    "# print str(a)\n",
    "b = []\n",
    "lists=[]\n",
    "for n in a:\n",
    "    print(n)\n",
    "    file_ = open(n).read()\n",
    "    # print file_\n",
    "    freq = len(re.findall(\"自然语言处理\",file_))\n",
    "    print(freq) # 单个文档的关键词词频\n",
    "    # file.close()\n",
    "    dir={\"file_dir\":n,\"freq\":freq}\n",
    "    lists.append(dir)\n",
    "    freq_ = b.append(freq)\n",
    "#print lists[0]\n",
    "lists.sort(key=getvalue,reverse=True)\n",
    "print(lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
